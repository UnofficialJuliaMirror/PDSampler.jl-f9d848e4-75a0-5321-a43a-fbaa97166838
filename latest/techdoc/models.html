<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Models · PDMP.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/><link href="../assets/partial.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>PDMP.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" action="../search.html"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../index.html">Introduction</a></li><li><a class="toctext" href="../aboutpdmp.html">About PDMP</a></li><li><span class="toctext">Examples</span><ul><li><a class="toctext" href="../examples/ex_gbps1.html">Global BPS</a></li><li><a class="toctext" href="../examples/ex_lbps1.html">Local BPS</a></li></ul></li><li><span class="toctext">Technical Documentation</span><ul><li><a class="toctext" href="structure.html">Code structure</a></li><li><a class="toctext" href="coretools.html">Core tools</a></li><li class="current"><a class="toctext" href="models.html">Models</a><ul class="internal"><li><a class="toctext" href="#Generic-model-1">Generic model</a></li><li><a class="toctext" href="#Multivariate-Gaussian-1">Multivariate Gaussian</a></li><li><a class="toctext" href="#Logistic-Regression-1">Logistic Regression</a></li><li><a class="toctext" href="#Probabilistic-Matrix-Factorisation-1">Probabilistic Matrix Factorisation</a></li></ul></li><li><a class="toctext" href="global.html">Global sampler</a></li><li><a class="toctext" href="local.html">Local sampler</a></li></ul></li><li><span class="toctext">Contributing</span><ul><li><a class="toctext" href="../contributing/addingexample.html">New example</a></li><li><a class="toctext" href="../contributing/addingfeature.html">New feature</a></li></ul></li></ul></nav><article id="docs"><header><nav><ul><li>Technical Documentation</li><li><a href="models.html">Models</a></li></ul><a class="edit-page" href="https://github.com/alan-turing-institute/PDMP.jl/tree/da2ef8c0cd2f0da64e05d30b298f21e7a8440a79/docs/src/techdoc/models.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Models</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="td-models-1" href="#td-models-1">Local sampler</a></h1><p>Link to the source files:</p><ul><li><p><a href="https://github.com/alan-turing-institute/PDMP.jl/blob/master/src/models/mvgaussian.jl"><code>models/mvgaussian.jl</code></a></p></li><li><p><a href="https://github.com/alan-turing-institute/PDMP.jl/blob/master/src/models/logreg.jl"><code>models/logreg.jl</code></a></p></li><li><p><a href="https://github.com/alan-turing-institute/PDMP.jl/blob/master/src/models/pmf.jl"><code>models/pmf.jl</code></a></p></li></ul><h2><a class="nav-anchor" id="Generic-model-1" href="#Generic-model-1">Generic model</a></h2><p>A model must be an immutable type with an associated <code>gradloglik</code> function. It is important this function be coded <em>as efficiently as possible</em> since it is called a large number of time in any simulation.</p><h2><a class="nav-anchor" id="Multivariate-Gaussian-1" href="#Multivariate-Gaussian-1">Multivariate Gaussian</a></h2><h3><a class="nav-anchor" id="Hierarchy-of-types-1" href="#Hierarchy-of-types-1">Hierarchy of types</a></h3><p>Multiple parametrisation are possible. Some being more efficient than others while some are maybe more intuitive than others.</p><pre><code class="language-none">MvGaussian (abstract)
| — MvGaussianStandard
| — MvDiagonalGaussian
| — MvGaussianCanon
| — MvGaussianNatural</code></pre><p>In the sequel we write <span>$\mu$</span> the mean, <span>$\Sigma$</span> the covariance matrix and <span>$\Omega$</span> the precision matrix. The different way to parametrise the distributions are as follows:</p><ul><li><p><code>MvGaussianStandard</code>, direct: <span>$(\mu, \Sigma)$</span>, indirect: (\Omega\mu,\Omega)</p></li><li><p><code>MvDiagonalGaussian</code>, direct: <span>$(\mu, (\sigma_i))$</span>, indirect: <span>$(\sigma_i^2)$</span></p></li><li><p><code>MvGaussianCanon</code>, direct: <span>$(\mu, \Omega)$</span>, indirect: <span>$(\Omega\mu)$</span></p></li><li><p><code>MvGaussianNatural</code>, direct: <span>$(\Omega\mu,-\Omega)$</span></p></li></ul><p>The preferred way is the &quot;canonical&quot; representation (most efficient).</p><p><strong>Note</strong>: &quot;direct&quot; means that these are the parameters passed to the constructor while &quot;indirect&quot; means that these values are computed when the constructor is called.</p><h3><a class="nav-anchor" id="Auxiliary-functions-1" href="#Auxiliary-functions-1">Auxiliary functions</a></h3><p>Internally, the types mentioned above are shortened to <code>MvGS</code>, <code>MvDG</code> etc. Then a number of simplifying functions are defined (these simplify the computation of the log-likelihood and gradient of the log-likelihood)</p><ul><li><p><code>mvg_mu</code> to recover <span>$\mu$</span></p></li><li><p><code>mvg_precmu</code> to recover <span>$\Omega\mu$</span></p></li><li><p><code>mvg_precmult</code> taking a point and multiplying it by <span>$\Omega$</span></p></li></ul><p><code>gradloglik</code> is then trivial to compute.</p><h2><a class="nav-anchor" id="Logistic-Regression-1" href="#Logistic-Regression-1">Logistic Regression</a></h2><p>The logistic regression considers a feature matrix <code>X</code>, a response <code>y</code>, the Lipschitz constant associated to it and dimensionality parameters.</p><h3><a class="nav-anchor" id="Auxiliary-functions-2" href="#Auxiliary-functions-2">Auxiliary functions</a></h3><p>A number of auxiliary functions are defined to prevent numerical instabilities and ensure that the computation of the log-likelihood and gradient of the log-likelihood can be expressed simply.</p><p>The <code>gradloglik_cv</code> considers a control-variate gradient developed around a given point (see <a href="https://arxiv.org/pdf/1701.04244.pdf">this paper</a> for more details).</p><p><strong>Note</strong>: the response is in <span>$\{-1,1\}$</span>.</p><h2><a class="nav-anchor" id="Probabilistic-Matrix-Factorisation-1" href="#Probabilistic-Matrix-Factorisation-1">Probabilistic Matrix Factorisation</a></h2><p>This model considers a normal distribution on every entry of a matrix <span>$r_{ij}$</span>:</p><p>\begin{equation} \mathcal N(r_{ij}; \langle u,v\rangle , \sigma^2) \end{equation}</p><p>The resulting intensity can be shown to be a truncated cubic for which we can in fact also do exact sampling.</p><p>The <code>pmf_case*</code> correspond to the various possible cases depending on where the roots of the cubic are.</p><footer><hr/><a class="previous" href="coretools.html"><span class="direction">Previous</span><span class="title">Core tools</span></a><a class="next" href="global.html"><span class="direction">Next</span><span class="title">Global sampler</span></a></footer></article></body></html>
